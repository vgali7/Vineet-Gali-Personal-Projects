{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lv1W6uRXg0xB"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pyZEvkYgGPs_",
    "outputId": "508a15b0-945d-4af6-8a0f-3179c601c0ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-16 14:22:27--  https://docs.google.com/uc?export=download&id=1fdGUg5fQl7FGUNYyK2hiVL84uc2EnzGo\n",
      "Resolving docs.google.com (docs.google.com)... 2607:f8b0:4002:c08::8a, 2607:f8b0:4002:c08::8b, 2607:f8b0:4002:c08::64, ...\n",
      "Connecting to docs.google.com (docs.google.com)|2607:f8b0:4002:c08::8a|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://drive.usercontent.google.com/download?id=1fdGUg5fQl7FGUNYyK2hiVL84uc2EnzGo&export=download [following]\n",
      "--2024-01-16 14:22:27--  https://drive.usercontent.google.com/download?id=1fdGUg5fQl7FGUNYyK2hiVL84uc2EnzGo&export=download\n",
      "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 2607:f8b0:4002:c0c::84, 74.125.138.132\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|2607:f8b0:4002:c0c::84|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 74864162 (71M) [application/octet-stream]\n",
      "Saving to: ‘data.csv’\n",
      "\n",
      "data.csv            100%[===================>]  71.40M  8.22MB/s    in 10s     \n",
      "\n",
      "2024-01-16 14:22:40 (7.04 MB/s) - ‘data.csv’ saved [74864162/74864162]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Download Spotify Million Song Dataset from https://drive.google.com/file/d/1fdGUg5fQl7FGUNYyK2hiVL84uc2EnzGo/view#\n",
    "!wget 'https://docs.google.com/uc?export=download&id=1fdGUg5fQl7FGUNYyK2hiVL84uc2EnzGo' -O data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rkaKXP7kgZH5",
    "outputId": "72d3786c-26b9-42c4-c69a-a6df2bce313c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p3SeGSWtujrQ",
    "outputId": "b13e9276-9751-4234-a9b9-234469c01ca6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zinCIxrPlRP2"
   },
   "source": [
    "Load in data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "EGG1IOBpgy7f",
    "outputId": "83d53e74-7080-400d-c357-7a4c7010cde1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\r\\nA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\r\\nTouch me gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\r\\nWhy I had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\r\\nA...  \n",
       "1  Take it easy with me, please  \\r\\nTouch me gen...  \n",
       "2  I'll never know why I had to go  \\r\\nWhy I had...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all = pd.read_csv(\"data.csv\")\n",
    "\n",
    "data_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lv1W6uRXg0xB"
   },
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FKNm8Ag1hIcz",
    "outputId": "da44b6f5-9678-4f5b-aab1-a45c6419d808"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Donna Summer        191\n",
       "Gordon Lightfoot    189\n",
       "Bob Dylan           188\n",
       "George Strait       188\n",
       "Loretta Lynn        187\n",
       "Cher                187\n",
       "Alabama             187\n",
       "Reba Mcentire       187\n",
       "Name: artist, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_data = data_all[\"artist\"].value_counts().nlargest(8)\n",
    "top_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "BuOQs7gDhgTs",
    "outputId": "52fb919b-f6f6-4c8b-e8f0-2ace152510bd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Calling All Angels</td>\n",
       "      <td>/a/alabama/calling+all+angels_20005071.html</td>\n",
       "      <td>Calling, calling all angels, oh I'm calling, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Can't Keep A Good Man Down</td>\n",
       "      <td>/a/alabama/cant+keep+a+good+man+down_20522156....</td>\n",
       "      <td>I thought it was forever  \\r\\nI thought it wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Carolina Mountain Dew</td>\n",
       "      <td>/a/alabama/carolina+mountain+dew_20175804.html</td>\n",
       "      <td>Somewhere in the mountains......... In norther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Christmas In Dixie</td>\n",
       "      <td>/a/alabama/christmas+in+dixie_20005147.html</td>\n",
       "      <td>By now in New York City, there's snow on the g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Christmas In Your Arms</td>\n",
       "      <td>/a/alabama/christmas+in+your+arms_20005047.html</td>\n",
       "      <td>All my friends are asking me where I plan to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50506</th>\n",
       "      <td>Reba Mcentire</td>\n",
       "      <td>Old Man River</td>\n",
       "      <td>/r/reba+mcentire/old+man+river_20114622.html</td>\n",
       "      <td>(ronny scaife, danny hogan)  \\r\\n  \\r\\nCool br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50507</th>\n",
       "      <td>Reba Mcentire</td>\n",
       "      <td>On My Own</td>\n",
       "      <td>/r/reba+mcentire/on+my+own_20114592.html</td>\n",
       "      <td>So many times I said it was forever  \\r\\nSaid ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50508</th>\n",
       "      <td>Reba Mcentire</td>\n",
       "      <td>One Child, One Day</td>\n",
       "      <td>/r/reba+mcentire/one+child+one+day_20114569.html</td>\n",
       "      <td>Three wise men, a shining star  \\r\\nA mother a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50509</th>\n",
       "      <td>Reba Mcentire</td>\n",
       "      <td>One Promise Too Late</td>\n",
       "      <td>/r/reba+mcentire/one+promise+too+late_20114392...</td>\n",
       "      <td>I would have waited forever  \\r\\nIf I'd known ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50510</th>\n",
       "      <td>Reba Mcentire</td>\n",
       "      <td>One Thin Dime</td>\n",
       "      <td>/r/reba+mcentire/one+thin+dime_20114664.html</td>\n",
       "      <td>Your mind's made up  \\r\\nYou're gonna leave  \\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1504 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              artist                        song  \\\n",
       "361          Alabama          Calling All Angels   \n",
       "362          Alabama  Can't Keep A Good Man Down   \n",
       "363          Alabama       Carolina Mountain Dew   \n",
       "364          Alabama          Christmas In Dixie   \n",
       "365          Alabama      Christmas In Your Arms   \n",
       "...              ...                         ...   \n",
       "50506  Reba Mcentire               Old Man River   \n",
       "50507  Reba Mcentire                   On My Own   \n",
       "50508  Reba Mcentire          One Child, One Day   \n",
       "50509  Reba Mcentire        One Promise Too Late   \n",
       "50510  Reba Mcentire               One Thin Dime   \n",
       "\n",
       "                                                    link  \\\n",
       "361          /a/alabama/calling+all+angels_20005071.html   \n",
       "362    /a/alabama/cant+keep+a+good+man+down_20522156....   \n",
       "363       /a/alabama/carolina+mountain+dew_20175804.html   \n",
       "364          /a/alabama/christmas+in+dixie_20005147.html   \n",
       "365      /a/alabama/christmas+in+your+arms_20005047.html   \n",
       "...                                                  ...   \n",
       "50506       /r/reba+mcentire/old+man+river_20114622.html   \n",
       "50507           /r/reba+mcentire/on+my+own_20114592.html   \n",
       "50508   /r/reba+mcentire/one+child+one+day_20114569.html   \n",
       "50509  /r/reba+mcentire/one+promise+too+late_20114392...   \n",
       "50510       /r/reba+mcentire/one+thin+dime_20114664.html   \n",
       "\n",
       "                                                    text  \n",
       "361    Calling, calling all angels, oh I'm calling, c...  \n",
       "362    I thought it was forever  \\r\\nI thought it wou...  \n",
       "363    Somewhere in the mountains......... In norther...  \n",
       "364    By now in New York City, there's snow on the g...  \n",
       "365    All my friends are asking me where I plan to s...  \n",
       "...                                                  ...  \n",
       "50506  (ronny scaife, danny hogan)  \\r\\n  \\r\\nCool br...  \n",
       "50507  So many times I said it was forever  \\r\\nSaid ...  \n",
       "50508  Three wise men, a shining star  \\r\\nA mother a...  \n",
       "50509  I would have waited forever  \\r\\nIf I'd known ...  \n",
       "50510  Your mind's made up  \\r\\nYou're gonna leave  \\...  \n",
       "\n",
       "[1504 rows x 4 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_all.loc[data_all[\"artist\"].isin(top_data.index)]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHzOJY47lU_u"
   },
   "source": [
    "Clean the text by removing all brackets and deleting content inside.\n",
    "This removes unnecessary lyrics such as adlibs, sound effects, etc.\n",
    "Remove line breaks as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bcVzDMojAVu",
    "outputId": "c3cda6b9-2456-442d-efd3-d67e392515f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of round brackets: 540\n",
      "Number of square brackets: 707\n"
     ]
    }
   ],
   "source": [
    "text_in_round_brackets = sum(list(data['text'].map(lambda s: re.findall(r'\\((.*?)\\)',s))), [])\n",
    "text_in_square_brackets = sum(list(data['text'].map(lambda s: re.findall(r'\\[(.*?)\\]',s))), [])\n",
    "\n",
    "print(f'Number of round brackets: {len(text_in_round_brackets)}')\n",
    "print(f'Number of square brackets: {len(text_in_square_brackets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W0Irze_BkHcn",
    "outputId": "116a5bc8-ef26-47d3-ba70-b7c0d3a9cb65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sometimes', 'she is the lady of the night', 'Oh, oh baby', 'Ah', 'I']\n",
      "['Chorus:', 'Chorus', 'Chorus:', 'Chorus', 'Chorus:']\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "print(random.choices(text_in_round_brackets, k=5))\n",
    "print(random.choices(text_in_square_brackets, k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qcdRey3vkKIy",
    "outputId": "4744825b-c9b1-4ca4-9982-ed7fbb48f7b6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Calling All Angels</td>\n",
       "      <td>/a/alabama/calling+all+angels_20005071.html</td>\n",
       "      <td>Calling, calling all angels, oh I'm calling, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Can't Keep A Good Man Down</td>\n",
       "      <td>/a/alabama/cant+keep+a+good+man+down_20522156....</td>\n",
       "      <td>I thought it was forever  I thought it would l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Carolina Mountain Dew</td>\n",
       "      <td>/a/alabama/carolina+mountain+dew_20175804.html</td>\n",
       "      <td>Somewhere in the mountains......... In norther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Christmas In Dixie</td>\n",
       "      <td>/a/alabama/christmas+in+dixie_20005147.html</td>\n",
       "      <td>By now in New York City, there's snow on the g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Christmas In Your Arms</td>\n",
       "      <td>/a/alabama/christmas+in+your+arms_20005047.html</td>\n",
       "      <td>All my friends are asking me where I plan to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50506</th>\n",
       "      <td>Reba Mcentire</td>\n",
       "      <td>Old Man River</td>\n",
       "      <td>/r/reba+mcentire/old+man+river_20114622.html</td>\n",
       "      <td>ronny scaife, danny hogan    Cool breeze on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50507</th>\n",
       "      <td>Reba Mcentire</td>\n",
       "      <td>On My Own</td>\n",
       "      <td>/r/reba+mcentire/on+my+own_20114592.html</td>\n",
       "      <td>So many times I said it was forever  Said our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50508</th>\n",
       "      <td>Reba Mcentire</td>\n",
       "      <td>One Child, One Day</td>\n",
       "      <td>/r/reba+mcentire/one+child+one+day_20114569.html</td>\n",
       "      <td>Three wise men, a shining star  A mother and a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50509</th>\n",
       "      <td>Reba Mcentire</td>\n",
       "      <td>One Promise Too Late</td>\n",
       "      <td>/r/reba+mcentire/one+promise+too+late_20114392...</td>\n",
       "      <td>I would have waited forever  If I'd known that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50510</th>\n",
       "      <td>Reba Mcentire</td>\n",
       "      <td>One Thin Dime</td>\n",
       "      <td>/r/reba+mcentire/one+thin+dime_20114664.html</td>\n",
       "      <td>Your mind's made up  You're gonna leave  Nothi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1504 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              artist                        song  \\\n",
       "361          Alabama          Calling All Angels   \n",
       "362          Alabama  Can't Keep A Good Man Down   \n",
       "363          Alabama       Carolina Mountain Dew   \n",
       "364          Alabama          Christmas In Dixie   \n",
       "365          Alabama      Christmas In Your Arms   \n",
       "...              ...                         ...   \n",
       "50506  Reba Mcentire               Old Man River   \n",
       "50507  Reba Mcentire                   On My Own   \n",
       "50508  Reba Mcentire          One Child, One Day   \n",
       "50509  Reba Mcentire        One Promise Too Late   \n",
       "50510  Reba Mcentire               One Thin Dime   \n",
       "\n",
       "                                                    link  \\\n",
       "361          /a/alabama/calling+all+angels_20005071.html   \n",
       "362    /a/alabama/cant+keep+a+good+man+down_20522156....   \n",
       "363       /a/alabama/carolina+mountain+dew_20175804.html   \n",
       "364          /a/alabama/christmas+in+dixie_20005147.html   \n",
       "365      /a/alabama/christmas+in+your+arms_20005047.html   \n",
       "...                                                  ...   \n",
       "50506       /r/reba+mcentire/old+man+river_20114622.html   \n",
       "50507           /r/reba+mcentire/on+my+own_20114592.html   \n",
       "50508   /r/reba+mcentire/one+child+one+day_20114569.html   \n",
       "50509  /r/reba+mcentire/one+promise+too+late_20114392...   \n",
       "50510       /r/reba+mcentire/one+thin+dime_20114664.html   \n",
       "\n",
       "                                                    text  \n",
       "361    Calling, calling all angels, oh I'm calling, c...  \n",
       "362    I thought it was forever  I thought it would l...  \n",
       "363    Somewhere in the mountains......... In norther...  \n",
       "364    By now in New York City, there's snow on the g...  \n",
       "365    All my friends are asking me where I plan to s...  \n",
       "...                                                  ...  \n",
       "50506  ronny scaife, danny hogan    Cool breeze on th...  \n",
       "50507  So many times I said it was forever  Said our ...  \n",
       "50508  Three wise men, a shining star  A mother and a...  \n",
       "50509  I would have waited forever  If I'd known that...  \n",
       "50510  Your mind's made up  You're gonna leave  Nothi...  \n",
       "\n",
       "[1504 rows x 4 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "data['text'] = data['text'].map(lambda s: re.sub(r'\\(|\\)', '', s))\n",
    "data['text'] = data['text'].map(lambda s: re.sub(r'\\[(.*?)\\] ', '', s))\n",
    "data['text'] = data['text'].map(lambda s: re.sub(r'\\r\\n|\\n', '', s))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3BJBm6-ninp"
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data['tokenized'] = data['text'].map(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJI1TYngoyWB"
   },
   "source": [
    "Remove prefixes/suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 902
    },
    "id": "XCEfqmuvn5xq",
    "outputId": "a2a9a9cc-7a50-4385-ad9e-1d4cd473d8b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 324661\n",
      "Number of unique tokens: 11530\n",
      "Number of unique stems: 7060\n"
     ]
    }
   ],
   "source": [
    "data['stop_words_removed'] = data['tokenized'].map(lambda x: [word for word in x if word not in (stop_words) and len(word) >= 3])\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "token_to_stem = {}\n",
    "token_count = 0\n",
    "\n",
    "for lst in data['tokenized']:\n",
    "    for token in lst:\n",
    "        token_count += 1\n",
    "        if token not in token_to_stem:\n",
    "            token_to_stem[token] = stemmer.stem(token)\n",
    "\n",
    "data['stemmed'] = data['stop_words_removed'].map(lambda lst: [token_to_stem[token] for token in lst])\n",
    "\n",
    "print('Number of tokens: {}'.format(token_count))\n",
    "print('Number of unique tokens: {}'.format(len(token_to_stem.keys())))\n",
    "print('Number of unique stems: {}'.format(len(set(token_to_stem.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stop_words_removed</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Calling All Angels</td>\n",
       "      <td>/a/alabama/calling+all+angels_20005071.html</td>\n",
       "      <td>Calling, calling all angels, oh I'm calling, c...</td>\n",
       "      <td>[Calling, calling, all, angels, oh, I, m, call...</td>\n",
       "      <td>[Calling, calling, angels, calling, calling, a...</td>\n",
       "      <td>[call, call, angel, call, call, angel, the, ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Can't Keep A Good Man Down</td>\n",
       "      <td>/a/alabama/cant+keep+a+good+man+down_20522156....</td>\n",
       "      <td>I thought it was forever  I thought it would l...</td>\n",
       "      <td>[I, thought, it, was, forever, I, thought, it,...</td>\n",
       "      <td>[thought, forever, thought, would, last, Gotta...</td>\n",
       "      <td>[thought, forev, thought, would, last, gotta, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Carolina Mountain Dew</td>\n",
       "      <td>/a/alabama/carolina+mountain+dew_20175804.html</td>\n",
       "      <td>Somewhere in the mountains......... In norther...</td>\n",
       "      <td>[Somewhere, in, the, mountains, In, northern, ...</td>\n",
       "      <td>[Somewhere, mountains, northern, Alabama, The,...</td>\n",
       "      <td>[somewher, mountain, northern, alabama, the, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Christmas In Dixie</td>\n",
       "      <td>/a/alabama/christmas+in+dixie_20005147.html</td>\n",
       "      <td>By now in New York City, there's snow on the g...</td>\n",
       "      <td>[By, now, in, New, York, City, there, s, snow,...</td>\n",
       "      <td>[New, York, City, snow, ground, And, Californi...</td>\n",
       "      <td>[new, york, citi, snow, ground, and, californi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Christmas In Your Arms</td>\n",
       "      <td>/a/alabama/christmas+in+your+arms_20005047.html</td>\n",
       "      <td>All my friends are asking me where I plan to s...</td>\n",
       "      <td>[All, my, friends, are, asking, me, where, I, ...</td>\n",
       "      <td>[All, friends, asking, plan, spend, holidays, ...</td>\n",
       "      <td>[all, friend, ask, plan, spend, holiday, peopl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50506</th>\n",
       "      <td>Reba Mcentire</td>\n",
       "      <td>Old Man River</td>\n",
       "      <td>/r/reba+mcentire/old+man+river_20114622.html</td>\n",
       "      <td>ronny scaife, danny hogan    Cool breeze on th...</td>\n",
       "      <td>[ronny, scaife, danny, hogan, Cool, breeze, on...</td>\n",
       "      <td>[ronny, scaife, danny, hogan, Cool, breeze, ri...</td>\n",
       "      <td>[ronni, scaif, danni, hogan, cool, breez, rive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50507</th>\n",
       "      <td>Reba Mcentire</td>\n",
       "      <td>On My Own</td>\n",
       "      <td>/r/reba+mcentire/on+my+own_20114592.html</td>\n",
       "      <td>So many times I said it was forever  Said our ...</td>\n",
       "      <td>[So, many, times, I, said, it, was, forever, S...</td>\n",
       "      <td>[many, times, said, forever, Said, love, would...</td>\n",
       "      <td>[mani, time, said, forev, said, love, would, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50508</th>\n",
       "      <td>Reba Mcentire</td>\n",
       "      <td>One Child, One Day</td>\n",
       "      <td>/r/reba+mcentire/one+child+one+day_20114569.html</td>\n",
       "      <td>Three wise men, a shining star  A mother and a...</td>\n",
       "      <td>[Three, wise, men, a, shining, star, A, mother...</td>\n",
       "      <td>[Three, wise, men, shining, star, mother, fath...</td>\n",
       "      <td>[three, wise, men, shine, star, mother, father...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50509</th>\n",
       "      <td>Reba Mcentire</td>\n",
       "      <td>One Promise Too Late</td>\n",
       "      <td>/r/reba+mcentire/one+promise+too+late_20114392...</td>\n",
       "      <td>I would have waited forever  If I'd known that...</td>\n",
       "      <td>[I, would, have, waited, forever, If, I, d, kn...</td>\n",
       "      <td>[would, waited, forever, known, could, shared,...</td>\n",
       "      <td>[would, wait, forev, known, could, share, live...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50510</th>\n",
       "      <td>Reba Mcentire</td>\n",
       "      <td>One Thin Dime</td>\n",
       "      <td>/r/reba+mcentire/one+thin+dime_20114664.html</td>\n",
       "      <td>Your mind's made up  You're gonna leave  Nothi...</td>\n",
       "      <td>[Your, mind, s, made, up, You, re, gonna, leav...</td>\n",
       "      <td>[Your, mind, made, You, gonna, leave, Nothing,...</td>\n",
       "      <td>[your, mind, made, you, gonna, leav, noth, say...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1504 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              artist                        song  \\\n",
       "361          Alabama          Calling All Angels   \n",
       "362          Alabama  Can't Keep A Good Man Down   \n",
       "363          Alabama       Carolina Mountain Dew   \n",
       "364          Alabama          Christmas In Dixie   \n",
       "365          Alabama      Christmas In Your Arms   \n",
       "...              ...                         ...   \n",
       "50506  Reba Mcentire               Old Man River   \n",
       "50507  Reba Mcentire                   On My Own   \n",
       "50508  Reba Mcentire          One Child, One Day   \n",
       "50509  Reba Mcentire        One Promise Too Late   \n",
       "50510  Reba Mcentire               One Thin Dime   \n",
       "\n",
       "                                                    link  \\\n",
       "361          /a/alabama/calling+all+angels_20005071.html   \n",
       "362    /a/alabama/cant+keep+a+good+man+down_20522156....   \n",
       "363       /a/alabama/carolina+mountain+dew_20175804.html   \n",
       "364          /a/alabama/christmas+in+dixie_20005147.html   \n",
       "365      /a/alabama/christmas+in+your+arms_20005047.html   \n",
       "...                                                  ...   \n",
       "50506       /r/reba+mcentire/old+man+river_20114622.html   \n",
       "50507           /r/reba+mcentire/on+my+own_20114592.html   \n",
       "50508   /r/reba+mcentire/one+child+one+day_20114569.html   \n",
       "50509  /r/reba+mcentire/one+promise+too+late_20114392...   \n",
       "50510       /r/reba+mcentire/one+thin+dime_20114664.html   \n",
       "\n",
       "                                                    text  \\\n",
       "361    Calling, calling all angels, oh I'm calling, c...   \n",
       "362    I thought it was forever  I thought it would l...   \n",
       "363    Somewhere in the mountains......... In norther...   \n",
       "364    By now in New York City, there's snow on the g...   \n",
       "365    All my friends are asking me where I plan to s...   \n",
       "...                                                  ...   \n",
       "50506  ronny scaife, danny hogan    Cool breeze on th...   \n",
       "50507  So many times I said it was forever  Said our ...   \n",
       "50508  Three wise men, a shining star  A mother and a...   \n",
       "50509  I would have waited forever  If I'd known that...   \n",
       "50510  Your mind's made up  You're gonna leave  Nothi...   \n",
       "\n",
       "                                               tokenized  \\\n",
       "361    [Calling, calling, all, angels, oh, I, m, call...   \n",
       "362    [I, thought, it, was, forever, I, thought, it,...   \n",
       "363    [Somewhere, in, the, mountains, In, northern, ...   \n",
       "364    [By, now, in, New, York, City, there, s, snow,...   \n",
       "365    [All, my, friends, are, asking, me, where, I, ...   \n",
       "...                                                  ...   \n",
       "50506  [ronny, scaife, danny, hogan, Cool, breeze, on...   \n",
       "50507  [So, many, times, I, said, it, was, forever, S...   \n",
       "50508  [Three, wise, men, a, shining, star, A, mother...   \n",
       "50509  [I, would, have, waited, forever, If, I, d, kn...   \n",
       "50510  [Your, mind, s, made, up, You, re, gonna, leav...   \n",
       "\n",
       "                                      stop_words_removed  \\\n",
       "361    [Calling, calling, angels, calling, calling, a...   \n",
       "362    [thought, forever, thought, would, last, Gotta...   \n",
       "363    [Somewhere, mountains, northern, Alabama, The,...   \n",
       "364    [New, York, City, snow, ground, And, Californi...   \n",
       "365    [All, friends, asking, plan, spend, holidays, ...   \n",
       "...                                                  ...   \n",
       "50506  [ronny, scaife, danny, hogan, Cool, breeze, ri...   \n",
       "50507  [many, times, said, forever, Said, love, would...   \n",
       "50508  [Three, wise, men, shining, star, mother, fath...   \n",
       "50509  [would, waited, forever, known, could, shared,...   \n",
       "50510  [Your, mind, made, You, gonna, leave, Nothing,...   \n",
       "\n",
       "                                                 stemmed  \n",
       "361    [call, call, angel, call, call, angel, the, ni...  \n",
       "362    [thought, forev, thought, would, last, gotta, ...  \n",
       "363    [somewher, mountain, northern, alabama, the, c...  \n",
       "364    [new, york, citi, snow, ground, and, californi...  \n",
       "365    [all, friend, ask, plan, spend, holiday, peopl...  \n",
       "...                                                  ...  \n",
       "50506  [ronni, scaif, danni, hogan, cool, breez, rive...  \n",
       "50507  [mani, time, said, forev, said, love, would, a...  \n",
       "50508  [three, wise, men, shine, star, mother, father...  \n",
       "50509  [would, wait, forev, known, could, share, live...  \n",
       "50510  [your, mind, made, you, gonna, leav, noth, say...  \n",
       "\n",
       "[1504 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSpShcBpz3qQ"
   },
   "source": [
    "Removing unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7rVJz_hviyZE"
   },
   "outputs": [],
   "source": [
    "data = data.drop(['link', 'tokenized', 'stop_words_removed', 'song', 'text'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSpShcBpz3qQ"
   },
   "source": [
    "Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "LH07CwBAfHGg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents, Length of Vector: (1504, 6868)\n"
     ]
    }
   ],
   "source": [
    "join_stemmed = data['stemmed'].map(lambda tokens: \" \".join(tokens))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorized = vectorizer.fit_transform(join_stemmed.values).toarray()\n",
    "data['vectorized'] = vectorized.tolist()\n",
    "\n",
    "vector_length = vectorized.shape\n",
    "print(\"Number of Documents, Length of Vector:\", vector_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiBCtZOspdMd"
   },
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "4ZIRVvV7pBEj",
    "outputId": "33caea56-aa96-4036-cbcc-90993f90803a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data, test_size=0.2, stratify = data['artist'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donna Summer        153\n",
      "Gordon Lightfoot    151\n",
      "George Strait       150\n",
      "Bob Dylan           150\n",
      "Reba Mcentire       150\n",
      "Cher                150\n",
      "Loretta Lynn        150\n",
      "Alabama             149\n",
      "Name: artist, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df['artist'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George Strait       38\n",
      "Bob Dylan           38\n",
      "Alabama             38\n",
      "Donna Summer        38\n",
      "Gordon Lightfoot    38\n",
      "Loretta Lynn        37\n",
      "Reba Mcentire       37\n",
      "Cher                37\n",
      "Name: artist, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test_df['artist'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>vectorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33891</th>\n",
       "      <td>George Strait</td>\n",
       "      <td>[you, say, want, talk, want, tri, but, way, wo...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34334</th>\n",
       "      <td>Gordon Lightfoot</td>\n",
       "      <td>[four, month, ago, april, daycoach, came, and,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4413</th>\n",
       "      <td>Donna Summer</td>\n",
       "      <td>[undercov, cop, car, came, screech, halt, bodi...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25795</th>\n",
       "      <td>Bob Dylan</td>\n",
       "      <td>[utter, idl, word, reprob, mind, cling, strang...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50459</th>\n",
       "      <td>Reba Mcentire</td>\n",
       "      <td>[thought, send, rose, for, reason, send, rose,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4401</th>\n",
       "      <td>Donna Summer</td>\n",
       "      <td>[perfect, love, never, find, say, goodby, left...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50443</th>\n",
       "      <td>Reba Mcentire</td>\n",
       "      <td>[love, take, patienc, job, that, mama, alway, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>Loretta Lynn</td>\n",
       "      <td>[know, see, touch, sun, pain, wet, get, hurt, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50485</th>\n",
       "      <td>Reba Mcentire</td>\n",
       "      <td>[daddi, use, tell, ran, fast, fall, hurt, but,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6747</th>\n",
       "      <td>George Strait</td>\n",
       "      <td>[nickel, dime, memori, wine, she, mind, the, o...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1203 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 artist                                            stemmed  \\\n",
       "33891     George Strait  [you, say, want, talk, want, tri, but, way, wo...   \n",
       "34334  Gordon Lightfoot  [four, month, ago, april, daycoach, came, and,...   \n",
       "4413       Donna Summer  [undercov, cop, car, came, screech, halt, bodi...   \n",
       "25795         Bob Dylan  [utter, idl, word, reprob, mind, cling, strang...   \n",
       "50459     Reba Mcentire  [thought, send, rose, for, reason, send, rose,...   \n",
       "...                 ...                                                ...   \n",
       "4401       Donna Summer  [perfect, love, never, find, say, goodby, left...   \n",
       "50443     Reba Mcentire  [love, take, patienc, job, that, mama, alway, ...   \n",
       "11996      Loretta Lynn  [know, see, touch, sun, pain, wet, get, hurt, ...   \n",
       "50485     Reba Mcentire  [daddi, use, tell, ran, fast, fall, hurt, but,...   \n",
       "6747      George Strait  [nickel, dime, memori, wine, she, mind, the, o...   \n",
       "\n",
       "                                              vectorized  \n",
       "33891  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "34334  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4413   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "25795  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "50459  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "...                                                  ...  \n",
       "4401   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "50443  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "11996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "50485  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "6747   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[1203 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfZS-TqTKjTY"
   },
   "source": [
    "# Traditional Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, train_df, test_df, algorithm=None):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.slider = slider\n",
    "        self.algorithm = algorithm\n",
    "        \n",
    "        self.classifier = None\n",
    "\n",
    "    def plot(self):\n",
    "        if self.algorithm == \"KNN\":\n",
    "            return\n",
    "        elif self.algorithm == \"Decision Tree\":\n",
    "            print(\"Decision Tree...\")\n",
    "            \n",
    "            plot_tree(self.classifier, filled=True)\n",
    "            self.classifier = DecisionTreeClassifier(max_depth=slider, random_state=100)\n",
    "            \n",
    "        elif self.algorithm == \"Random Forest\":\n",
    "            print(\"Random Forest...\")\n",
    "            \n",
    "            for i, tree in enumerate(self.classifier.estimators_):\n",
    "                plot_tree(tree, filled=True, max_depth=5)\n",
    "                plt.title(f\"Tree {i + 1}\")\n",
    "                plt.show()\n",
    "                \n",
    "        elif self.algorithm == \"Gradient Boosting\":\n",
    "            return\n",
    "\n",
    "    def set_classifier(self, slider):\n",
    "        self.slider = slider\n",
    "        \n",
    "        if self.algorithm == \"KNN\":\n",
    "            self.classifier = KNeighborsClassifier(n_neighbors=slider)\n",
    "            self.slider_name = \"Number of neighbors: \"\n",
    "            \n",
    "        elif self.algorithm == \"Decision Tree\":\n",
    "            self.classifier = DecisionTreeClassifier(max_depth=slider, random_state=100)\n",
    "            self.slider_name = \"Maximum Depth: \"\n",
    "            \n",
    "        elif self.algorithm == \"Random Forest\":\n",
    "            self.classifier = RandomForestClassifier(n_estimators=slider, random_state=100)\n",
    "            self.slider_name = \"Number of estimators: \"\n",
    "            \n",
    "        elif self.algorithm == \"Gradient Boosting\":\n",
    "            self.classifier = GradientBoostingClassifier(n_estimators=slider, learning_rate=0.05, random_state=100)\n",
    "            self.slider_name = \"Number of estimators: \"\n",
    "        \n",
    "\n",
    "    def train(self, slider_val, plot=False):\n",
    "        print(\"Computing...\")\n",
    "        self.set_classifier(slider_val)\n",
    "        \n",
    "        train_vec = np.vstack(self.train_df['vectorized'])\n",
    "        test_vec = np.vstack(self.test_df['vectorized'])\n",
    "    \n",
    "        self.classifier.fit(train_vec, self.train_df['artist'])\n",
    "        \n",
    "        predicted = self.classifier.predict(test_vec)\n",
    "        classifier_accuracy = accuracy_score(test_df['artist'], predicted)\n",
    "        classifier_accuracy_percentage = round(classifier_accuracy * 100, 2)\n",
    "    \n",
    "        labels = test_df['artist'].unique()\n",
    "        matrix = confusion_matrix(test_df['artist'], predicted, labels=labels, normalize='true')\n",
    "        classifier_disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=labels)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(f\"{self.slider_name}: {self.slider}\")\n",
    "        print(f\"{self.algorithm} Accuracy: {classifier_accuracy_percentage}%\")\n",
    "    \n",
    "        classifier_disp.plot(xticks_rotation=75, values_format='.2f', colorbar = False)\n",
    "        plt.title(f'{self.algorithm} Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "        if plot:\n",
    "            self.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lv1W6uRXg0xB"
   },
   "source": [
    "# Traditional Machine Learning Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781c3a6533d74fc3a64e43f7e01bc1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Algorithm', options=('KNN', 'Decision Tree', 'Random Forest', 'Gra…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "def visualize_ml(algorithm):\n",
    "    classifier.algorithm = algorithm\n",
    "    \n",
    "    if algorithm == \"KNN\":\n",
    "        interact(classifier.train, slider_val=slider)\n",
    "        \n",
    "    elif algorithm == \"Decision Tree\":\n",
    "        interact(classifier.train, slider_val=slider)\n",
    "                \n",
    "    elif algorithm == \"Random Forest\":\n",
    "        interact(classifier.train, slider_val=slider)\n",
    "\n",
    "    elif algorithm == \"Gradient Boosting\":\n",
    "        interact(classifier.train, slider_val=slider)\n",
    "\n",
    "\n",
    "slider = ipywidgets.IntSlider(value=1, min=1, max=100, step=3, description='Value:')\n",
    "\n",
    "algorithms = [\"KNN\", \"Decision Tree\", \"Random Forest\", \"Gradient Boosting\"]\n",
    "algorithm_dropdown = ipywidgets.Dropdown(options=algorithms, value=algorithms[0], description='Algorithm')\n",
    "\n",
    "\n",
    "classifier = Classifier(train_df, test_df)\n",
    "\n",
    "interactive_plot = interact(visualize_ml, algorithm=algorithm_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfZS-TqTKjTY"
   },
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z7dxN7X5S5P5",
    "outputId": "334bd21b-b8b9-407a-80e4-862ea0aa40f5"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lv1W6uRXg0xB"
   },
   "source": [
    "### Feature Engineering Extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "Q6qRHqM_KnHG"
   },
   "outputs": [],
   "source": [
    "PADDING_VALUE = 0\n",
    "UNK_VALUE = 1\n",
    "\n",
    "def generate_vocab(df, min_count):\n",
    "    vocab = { \"\": PADDING_VALUE, \"UNK\": UNK_VALUE}\n",
    "\n",
    "    counter = Counter()\n",
    "    for word in df['stemmed']:\n",
    "        counter.update(word)\n",
    "    \n",
    "    id = 2\n",
    "    for token, count in counter.items():\n",
    "        if count > min_count:\n",
    "            vocab[token] = id\n",
    "            id += 1\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def generate_labels(df):\n",
    "    artists = train_df['artist'].unique()\n",
    "    labels = {artist: index for index, artist in enumerate(artists)}\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "def collate_fn(batch, padding_value=PADDING_VALUE): # Dataloader parameter function to batch data\n",
    "    \n",
    "    tokens, labels = zip(*batch)\n",
    "    padded_tokens = pad_sequence(tokens, batch_first=True, padding_value=padding_value).long().to(device)\n",
    "    labels_tensor = torch.tensor(labels).long().to(device)\n",
    "\n",
    "    return padded_tokens, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorDataset(Dataset): \n",
    "\n",
    "    def __init__(self, df, labels):\n",
    "        self.df = df\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        vector = torch.tensor(row['vectorized']).float().to(device)\n",
    "        label = torch.tensor(self.labels[row['artist']]).long().to(device)\n",
    "\n",
    "        return vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricDataset(Dataset): \n",
    "\n",
    "    def __init__(self, df, vocab, labels, max_length=200):\n",
    "        self.vocab = vocab\n",
    "        self.labels = labels\n",
    "        self.df = df\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        ids = []\n",
    "\n",
    "        for token in row['stemmed'][:self.max_length]:\n",
    "            if token in self.vocab:\n",
    "                ids.append(self.vocab[token])\n",
    "            else:\n",
    "                ids.append(self.vocab['UNK'])\n",
    "            \n",
    "        tokenized_tensor = torch.tensor(ids).long()\n",
    "        label = torch.tensor(self.labels[row['artist']]).long()\n",
    "\n",
    "        return tokenized_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "vocab = generate_vocab(train_df, 3) #generate unique vocab map\n",
    "labels = generate_labels(train_df) #generate unique label map\n",
    "\n",
    "# Create Loaders for Feed-Forward and Recurrent Model\n",
    "train_vector_dataset = VectorDataset(train_df, labels)\n",
    "test_vector_dataset = VectorDataset(test_df, labels)\n",
    "\n",
    "train_vector_sampler = RandomSampler(train_vector_dataset)\n",
    "test_vector_sampler = RandomSampler(test_vector_dataset)\n",
    "\n",
    "train_vector_loader = DataLoader(train_vector_dataset, batch_size=BATCH_SIZE, sampler=train_vector_sampler)\n",
    "test_vector_loader = DataLoader(test_vector_dataset, batch_size=BATCH_SIZE, sampler=test_vector_sampler)\n",
    "\n",
    "#Create Loaders for Neural Embedding\n",
    "train_lyric_dataset = LyricDataset(train_df, vocab, labels)\n",
    "test_lyric_dataset = LyricDataset(test_df, vocab, labels)\n",
    "\n",
    "train_lyric_sampler = RandomSampler(train_lyric_dataset)\n",
    "test_lyric_sampler = RandomSampler(test_lyric_dataset)\n",
    "\n",
    "train_lyric_loader = DataLoader(train_lyric_dataset, batch_size=BATCH_SIZE, sampler=train_lyric_sampler, collate_fn=collate_fn)\n",
    "test_lyric_loader = DataLoader(test_lyric_dataset, batch_size=BATCH_SIZE, sampler=test_lyric_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lv1W6uRXg0xB"
   },
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "c6hK30vuW0de"
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optim, iterator):\n",
    "    model.train() #set to training mode\n",
    "    total_correct, total_loss, num_examples = 0, 0, 0\n",
    "    \n",
    "    for input_data, label in iterator:\n",
    "        \n",
    "        scores = model(input_data) #forward pass\n",
    "        predictions = scores.argmax(dim=1)\n",
    "\n",
    "        correct = torch.sum(predictions == label)\n",
    "        loss = criterion(scores, label)\n",
    "\n",
    "        optim.zero_grad() #clear gradient\n",
    "        loss.backward() #compute gradient\n",
    "        optim.step() #update parameters\n",
    "\n",
    "        total_correct += correct\n",
    "        total_loss += loss.item()\n",
    "        num_examples += len(label)\n",
    "\n",
    "    accuracy = 100. * total_correct / num_examples\n",
    "    avg_loss = total_loss / num_examples\n",
    "\n",
    "    return accuracy, avg_loss \n",
    "\n",
    "\n",
    "def evaluate(model, criterion, iterator):\n",
    "    model.eval() #set to evaluation mode\n",
    "    total_correct, total_loss, num_examples = 0, 0, 0\n",
    "\n",
    "    for input_data, label in iterator:\n",
    "        \n",
    "        scores = model(input_data)\n",
    "        predictions = scores.argmax(dim=1)\n",
    "\n",
    "        correct = torch.sum(predictions == label)\n",
    "        loss = criterion(scores, label)\n",
    "\n",
    "        total_correct += correct\n",
    "        total_loss += loss.item()\n",
    "        num_examples += len(label)\n",
    "\n",
    "\n",
    "    accuracy = 100. * total_correct / num_examples\n",
    "    avg_loss = total_loss / num_examples\n",
    "\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "\n",
    "def train_loop(model, criterion, optim, train_loader, test_loader, epochs):\n",
    "    best_test_acc = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        train_acc, train_loss = train(model, criterion, optim, train_loader)\n",
    "        test_acc, test_loss = evaluate(model, criterion, test_loader)\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "            print(f\"Highest Accuracy during epoch {epoch}: {test_acc}\")\n",
    "            best_test_acc = test_acc\n",
    "            torch.save(model.state_dict(), \"state.pth\")\n",
    "\n",
    "    model.load_state_dict(torch.load(\"state.pth\"))\n",
    "\n",
    "\n",
    "def predict_plot_confusion_matrix(model, iterator, test_df):\n",
    "    model.eval()\n",
    "    actuals, predictions = [], []\n",
    "\n",
    "    for input_data, label in iterator:\n",
    "        \n",
    "        scores = model(input_data)\n",
    "        preds = scores.argmax(dim=1)\n",
    "        predictions.extend(preds.tolist())\n",
    "\n",
    "        actuals.extend(label.tolist())\n",
    "\n",
    "    actuals, predictions = np.array(actuals), np.array(predictions)\n",
    "    \n",
    "    accuracy = np.sum(actuals == predictions) / len(actuals)\n",
    "    accuracy_percentage = str(round(accuracy * 100, 2)) + \"%\"\n",
    "\n",
    "    labels = test_df['artist'].unique()\n",
    "    matrix = confusion_matrix(actuals, predictions, normalize='true')\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=labels)\n",
    "\n",
    "    return disp, accuracy_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lv1W6uRXg0xB"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfZS-TqTKjTY"
   },
   "source": [
    "### Feed Forward Nerual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardModel(nn.Module):\n",
    "    def __init__(self, input_size, num_labels, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=input_size, out_features=50),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=50, out_features=50),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=50, out_features=num_labels),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward_model = FeedForwardModel(input_size=6868, num_labels=len(labels)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfZS-TqTKjTY"
   },
   "source": [
    "### Nerual Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "eCe0IcqvUAUi"
   },
   "outputs": [],
   "source": [
    "class NeuralEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels, embedding_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim, out_features=50),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=50, out_features=50),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=50, out_features=num_labels),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x.long())\n",
    "        averaged = embedded.mean(dim = 1)\n",
    "        probabilties = self.classifier(averaged)\n",
    "\n",
    "        return probabilties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Ia-qTGCXZoCQ"
   },
   "outputs": [],
   "source": [
    "neural_embedding_model = NeuralEmbedding(vocab_size=len(vocab), num_labels=len(labels), embedding_dim = 200).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfZS-TqTKjTY"
   },
   "source": [
    "### Recurrent Nerual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "YCeDW5gmaQvb"
   },
   "outputs": [],
   "source": [
    "class RecurrentModel(nn.Module):\n",
    "    def __init__(self, input_size, num_labels, hidden_size, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size, out_features=50),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=50, out_features=50),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=50, out_features=num_labels),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, memory = self.lstm(x)\n",
    "        \n",
    "        return self.classifier(lstm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "GKQbT8L9kQNY"
   },
   "outputs": [],
   "source": [
    "recurrent_model = RecurrentModel(input_size=6868, num_labels=len(labels), hidden_size=30).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfZS-TqTKjTY"
   },
   "source": [
    "# Neural Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralModelTrainer:\n",
    "    def __init__(self, model=None, criterion=None, train_loader=None, test_loader=None, lr=0.005, epochs=10):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.optimizer_type = None\n",
    "        self.model_title = None\n",
    "        self.optimizer_title = None\n",
    "\n",
    "    def optimize(self, optimizer_type):\n",
    "        self.optimizer_title = optimizer_type\n",
    "        if optimizer_type ==  \"SGD\":\n",
    "            self.optimizer_type = optim.SGD(self.model.parameters(), lr=0.005, momentum=0.9)\n",
    "        elif optimizer_type ==  \"RMSprop\":\n",
    "            self.optimizer_type = optim.RMSprop(self.model.parameters(), lr=0.005)\n",
    "        elif optimizer_type ==  \"Adam\":\n",
    "            self.optimizer_type = optim.Adam(self.model.parameters(), lr=0.005)\n",
    "\n",
    "    def train_and_evaluate(self, optimizer_type):\n",
    "        self.optimize(optimizer_type)\n",
    "        \n",
    "        print(\"Calculating...\")\n",
    "        train_loop(self.model, self.criterion, self.optimizer_type, self.train_loader, self.test_loader, self.epochs)\n",
    "        disp, accuracy = predict_plot_confusion_matrix(self.model, self.test_loader, test_df)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(f\"{self.optimizer_title} Optimizer Accuracy: {accuracy}\")\n",
    "        disp.plot(xticks_rotation=75, values_format='.2f', colorbar=False)\n",
    "        plt.title(f'{self.model_title} Confusion Matrix')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lv1W6uRXg0xB"
   },
   "source": [
    "# Deep Learning Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d29724293044719eed4fc23cbf94b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Algorithm', options=('Feed Forward Neural Network', 'Neural Embedd…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_dl(algorithm):\n",
    "    neural_model.model_title = algorithm\n",
    "    \n",
    "    if algorithm == \"Feed Forward Neural Network\":\n",
    "        neural_model.model = feed_forward_model\n",
    "        neural_model.train_loader = train_vector_loader\n",
    "        neural_model.test_loader = test_vector_loader\n",
    "        \n",
    "        interact(neural_model.train_and_evaluate, optimizer_type=optimizer_dropdown)\n",
    "        \n",
    "    elif algorithm == \"Neural Embedding\":\n",
    "        neural_model.model = neural_embedding_model\n",
    "        neural_model.train_loader = train_lyric_loader\n",
    "        neural_model.test_loader = test_lyric_loader\n",
    "        \n",
    "        interact(neural_model.train_and_evaluate, optimizer_type=optimizer_dropdown)\n",
    "    \n",
    "    elif algorithm == \"Recurrent Neural Network\":\n",
    "        neural_model.model = recurrent_model\n",
    "        neural_model.train_loader = train_vector_loader\n",
    "        neural_model.test_loader = test_vector_loader\n",
    "        \n",
    "        interact(neural_model.train_and_evaluate, optimizer_type=optimizer_dropdown)\n",
    "\n",
    "\n",
    "optimizers = [\"SGD\", \"RMSprop\", \"Adam\"]\n",
    "optimizer_dropdown = ipywidgets.Dropdown(options=optimizers, value=optimizers[2], description='Optimizer')\n",
    "\n",
    "algorithms = [\"Feed Forward Neural Network\", \"Neural Embedding\", \"Recurrent Neural Network\"]\n",
    "algorithm_dropdown = ipywidgets.Dropdown(options=algorithms, value=algorithms[0], description='Algorithm')\n",
    "\n",
    "\n",
    "neural_model = NeuralModelTrainer()\n",
    "neural_model.criterion = nn.CrossEntropyLoss()\n",
    "neural_model.optimizer_dropdown = optimizer_dropdown\n",
    "\n",
    "interactive_plot = interact(visualize_dl, algorithm=algorithm_dropdown)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
